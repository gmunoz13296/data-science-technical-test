# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16_ZvJCj6zdMniutI2pik1eXmoJ0uONZY
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# -----------------------------
# 1️⃣ Definir función FizzBuzz
# -----------------------------
def fizzbuzz_label(n):
    if n % 3 == 0 and n % 5 == 0:
        return "FizzBuzz"
    elif n % 3 == 0:
        return "Fizz"
    elif n % 5 == 0:
        return "Buzz"
    else:
        return "None"

# -----------------------------
# 2️⃣ Construir dataset de entrenamiento
# -----------------------------
N = 1000
numbers = np.arange(1, N+1)
labels = np.array([fizzbuzz_label(n) for n in numbers])

X = pd.DataFrame({
    'number': numbers,
    'mod3': numbers % 3,
    'mod5': numbers % 5
})
y = labels
X_encoded = pd.get_dummies(X, columns=['mod3','mod5'], drop_first=False)

# -----------------------------
# 3️⃣ Dividir en entrenamiento y prueba
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# -----------------------------
# 4️⃣ Elegir un modelo de clasificación cualquiera como base
# -----------------------------
initial_model = RandomForestClassifier(n_estimators=100, random_state=42)
initial_model.fit(X_train, y_train)

# Probarlo con números 1-100
X_100 = pd.DataFrame({
    'number': np.arange(1,101),
    'mod3': np.arange(1,101) % 3,
    'mod5': np.arange(1,101) % 5
})
X_100_encoded = pd.get_dummies(X_100, columns=['mod3','mod5'], drop_first=False)
X_100_encoded = X_100_encoded.reindex(columns=X_train.columns, fill_value=0)

y_true_100 = np.array([fizzbuzz_label(n) for n in range(1,101)])
y_pred_initial = initial_model.predict(X_100_encoded)
accuracy_initial = accuracy_score(y_true_100, y_pred_initial)
print(f"Accuracy del modelo inicial (Random Forest) en 1-100: {accuracy_initial:.4f}")

# -----------------------------
# 5️⃣ Validación cruzada con varios modelos
# -----------------------------
classifiers = {
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Logistic Regression": LogisticRegression(max_iter=1000)
}

kf = KFold(n_splits=10, shuffle=True, random_state=42)
cv_results = {}

print("\nResultados de 10-fold cross-validation:")
for name, clf in classifiers.items():
    scores = cross_val_score(clf, X_train, y_train, cv=kf, scoring='accuracy')
    cv_results[name] = scores.mean()
    print(f"{name}: Accuracy promedio = {scores.mean():.4f}")

# -----------------------------
# 6️⃣ Seleccionar mejor modelo según CV
# -----------------------------
best_model_name = max(cv_results, key=cv_results.get)
best_model = classifiers[best_model_name]
print(f"\nMejor modelo según CV: {best_model_name}")

# Comparar con el modelo inicial
if best_model_name == "Random Forest":
    print("El modelo inicial elegido (Random Forest) es efectivamente el mejor según CV.")
else:
    print(f"El modelo inicial no era el mejor; se recomienda usar {best_model_name} para mejorar precisión.")

# Entrenar el mejor modelo con todos los datos de entrenamiento
best_model.fit(X_train, y_train)

# -----------------------------
# 7️⃣ Probar el mejor modelo con números 1-100
# -----------------------------
y_pred_best = best_model.predict(X_100_encoded)
accuracy_best = accuracy_score(y_true_100, y_pred_best)
print(f"\nAccuracy del mejor modelo en números 1-100: {accuracy_best:.4f}")

# -----------------------------
# 8️⃣ Justificación
# -----------------------------
print("\nJustificación del modelo seleccionado:")
if best_model_name == "Random Forest":
    print("Random Forest captura reglas discretas de divisibilidad sin necesidad de linealidad.")
elif best_model_name == "SVM":
    print("SVM funciona bien si los residuos mod3 y mod5 son codificados adecuadamente.")
elif best_model_name == "KNN":
    print("KNN predice según vecinos más cercanos, útil para patrones pequeños.")
else:
    print("Regresión Logística puede funcionar bien con one-hot encoding, pero es lineal y depende de la codificación.")